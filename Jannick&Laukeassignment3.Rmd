---
title: "Computational Inference with R - Assignment 3"
author: "Jannick Akkermans & Lauke Stoel"
date: "13 november 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1: Emperical power calculation

#1

```{r}
set.seed(123) #set a seed so that the result will be the same every time
control_group <- rnorm(50, mean = 150, sd = 15) #sample 50 observations from a normal distribution with a mean of 150 and a standard deviation of 15 to be the control group
experimental_group <- rnorm(50, mean = 160, sd = 15) #sample 50 observations from a normal distribution with a mean of 160 and a standard deviation of 15 to be the experimental group
t.test(control_group, experimental_group, alternative = "less")
```

Our null hypothesis is that there is no difference between the control group and the experimental group. Our alternative hypothesis is that the experimental group on average scores higher than the control group. Our expectation is based on past research. We will therefore perform a one-sided t-test using the in-built `t.test()` function. The p-value from this test is 2.425e-05. Since this is lower than the alpha-level of 0.05, we reject the null hypothesis. Therefore, there is statistical evidence that the experimental group on average scores higher than the control group.

#2

```{r}
library(ggplot2)
set.seed(123) #set a seed so that the results will be the same every time the code block is run
N <- 1000 #set the number of bootstrap replications
simulateTtest <- function(N, n1, n2, mu1, mu2, sd1, sd2) {
  p_values <- numeric(N) #create a vector of 0's whose length is equal to N
  for (i in 1:N) { #simulate 1000 t-tests
    control_group <- rnorm(n1, mean = mu1, sd = sd1) #sample the control group
    experimental_group <- rnorm(n2, mean = mu2, sd = sd2) #sample the experimental group
    p_values[i] <- t.test(control_group, experimental_group)$p.value #perform a t-test and save the p-value
  }
  dx <- density(p_values, adjust = 10)
  plot(dx, xlab = "p-value", ylab = "Probability density", main = "Distribution of the simulated p-values"); polygon(c(dx$x[dx$x < 0.05], 0.05), c(dx$y[dx$x < 0.05], 0.05), col = rgb(1, 0, 0, alpha = 0.5), border = "red", main = "") #create a density plot that shows the distribution of the p-values and color the area that is lower than 0.05
  power <- mean(p_values < 0.05) #calculate the emperical power based on all the bootstrapped p-values
  list(Simulated_power = power, True_power = power.t.test(50, delta = 10, sd = 15, sig.level = 0.05, type = "two.sample", alternative = "one.sided")$power) #return a list that contains the emperical power and the true power
}

simulateTtest(N, 50, 50, 150, 160, 15, 15)
```
Power is defined as the probability of correctly rejecting the null hypothesis. In this exercise, a simulated value for power is calculated as the proportion of p-values lower than 0.05. The area that is associated with this proportion can be seen in the plot above, where the area with p-values lower than 0.05 is coloured red. The simulation returned a power value of 0.929. The function `power.t.test()` returns a power value of 0.952 which is higher than our simulated power value. This is most likely due to some correction applied in the `power.t.test()` function. In our simulation, every p-value that was lower than 0.05 contributed to the value of the simulated power. However, there could also be some Type-II error cases in this simulated value. This is probably accounted for by the `power.t.test()` function.

## Part 2: Resampling techniques

#Question 1

Firstly, to determine which resampling technique for hypothesis testing we should use, we should check the data for outliers. Bootstrapping does not rely on assumptions of an underlying distribution, but is is more sensitive to outliers than the permutation test. Additionally, it is less exact than the permutation test when you have small samples like in this question. Nevertheless, bootstrapping does have better generalizability than the permutation test.

```{r}
csfi <- c(2,5,5,6,6,7,8,9) #data from the computer-simulated flight instructions group
tfi <- c(1,1,2,3,3,4,5,7,7,8) #data from the traditional flight instructions group

var(csfi) #calculate the variance of the computer-simulated flight instructions group -> 4.57
var(tfi) #calculate the variance of the traditional flight instructions group -> 6.54

boxplot(csfi); boxplot(tfi) #create boxplots for both groups. These are used to check outliers
```

There are no outliers present in the data as can be seen in the boxplots. Therefore, we decided to use bootstrapping in light of the better generalizability.

```{r}
bootstrapTest <- function(group1, group2, B) {
  sample_difference <- mean(group1) - mean(group2) #The difference between the two groups as they are now. This difference is used to calculate the emperical p-value
  N <- length(group1) #size of group 1
  M <- length(group2) #size of group 2
  differences <- numeric(B) #create a vector with 0's with a length equal to the number of repetitions
  for (i in 1:B) {
    bootstrap_sample <- sample(c(group1, group2), replace = TRUE) #sample WITH replacement from the original sample
    tgroup1 <- bootstrap_sample[1:N] #assign the first N sampled observations to the first group
    tgroup2 <- bootstrap_sample[(N+1):(N+M)] #assign the remaining sampled observations to the second group
    differences[i] <- mean(tgroup1) - mean(tgroup2) #calcuate the mean difference between the groups and assign it to the differences vector
  }
  emp_p <- mean(abs(differences) > abs(sample_difference)) #calculate the proportion of absolute mean differences that is greater than the sample mean differences. We use the absolute value since we want to calculate a two-sided p-value.
  hist(differences); abline(v = sample_difference, lwd = 2, col = "red") #create a histogram of the distribution of the mean differences. Draw a red vertical line at the position of the observed mean difference
  list(Sample_difference = sample_difference, Distribution_mean = mean(differences), Distribution_sd= sd(differences), P_value = emp_p) #return a list with the observed mean difference, the mean of the bootstrap distribution, the standard deviation of the boostrap distribution and the empericial p-value
}

bootstrapTest(csfi, tfi, 100000)
```

#Question 2

When you do not want to use the `sample()` function to draw random samples, you could instead generate n random integers. These integers represent the cases from the combined sample that need to be in the random sample. This can be achieved by using the function `runif()` combined with the `floor()` function. This will generate n random integers, possibly containing some duplicates. Therefore, this procedure essentially mimics the bootstrapping approach.

```{r}
bootstrapTestnosample <- function(group1, group2, B) {
  sample_difference <- mean(group1) - mean(group2) #The difference between the two groups as they are now. This difference is used to calculate the emperical p-value
  N <- length(group1) #size of group 1
  M <- length(group2) #size of group 2
  differences <- numeric(B) #create a vector with 0's with a length equal to the number of repetitions
  combined_sample <- c(group1, group2) #combine the data of both groups into one group
  for (i in 1:B) {
    indices <- floor(runif(18, min=1, max=19)) #generate 18 random integers representing the indices of the observations to be selected
    bootstrap_sample <- combined_sample[indices] #sample the observations based on the randomly generated indices
    tgroup1 <- bootstrap_sample[1:N] #assign the first N sampled units to the first group
    tgroup2 <- bootstrap_sample[(N+1):(N+M)] #assign the remaining sampled units to the second group
    differences[i] <- mean(tgroup1) - mean(tgroup2) #calculate the mean difference between the groups and assign it to the differences vector
  }
  emp_p <- mean(abs(differences) > abs(sample_difference)) #calculate the proportion of absolute mean differences that is greater than the sample mean differences. We use the absolute value since we want to calculate a two-sided p-value.
  hist(differences); abline(v = sample_difference, lwd = 2, col = "red") #create a histogram of the distribution of the mean differences. Draw a red vertical line at the position of the observed mean difference
  list(Sample_difference = sample_difference, Distribution_mean = mean(differences), Distribution_sd= sd(differences), P_value = emp_p) #return a list with the observed mean difference, the mean of the bootstrap distribution, the standard deviation of the boostrap distribution and the empericial p-value
}

bootstrapTestnosample(csfi, tfi, 100000)
```

The results from the approach that did not use the `sample()` function are very close to the results from the approach that did use the `sample()` function. Therefore, it is possible to achieve the same results.

#Question 3

When talking about resampling methods for estimation, there are three options to choose from: bootstrapping, the jackknife approach, and the jackknife-after-bootstrap approach. Bootstrapping generally works well but can become computationally heavy. Jackknife produces lower bias, but is not useful for non-smooth statistics. According to the book by Rizzo, jackknife-after-bootstrap produces lowest standard error.

Generally, bootstrap methods are often used when the sample data is the only available data. This is the case for this question. Additionally, bootstrap methods can be used to calculate the bias in the estimation and to provide an estimate of the standard error of the bootstrap distribution. The jackknife-after-bootstrap can only provide an estimate of the standard error of it's distribution. Since we believe bias is a relevant aspect of estimation, we decided to use bootstrapping as a resampling technique for estimation. In addition, we will provide a 95% confidence interval of the bootstrap distribution.

```{r}
set.seed(123)
bootstrapEstimation <- function(csfi, tfi, B) {
  sample_difference <- mean(csfi) - mean(tfi) #calculate the observed mean difference between the groups
  differences <- numeric(B) #create vector of zeros equal in length to the number of replications
  for (i in 1:B) {
    csfi_sample <- sample(csfi, replace = TRUE) #sample with replacement from the experimental group
    tfi_sample <- sample(tfi, replace = TRUE) #sample with replacement from the control group
    differences[i] <- mean(csfi_sample) - mean(tfi_sample) #calculate the mean difference between the groups and store it inside the differences vector
  }
  bias <- mean(differences - sample_difference) #calculate the bias of the estimate
  se <- sd(differences) #calculate the standard error of the bootstrap distribution
  list(Estimated_mean_difference = mean(differences), Bias = bias, Standard_Error = se) #return a list that contains the mean of the bootstrap distribution, the bias of the estimated mean difference, and the standard error of the bootstrap distribution
}

estimates <- bootstrapEstimation(csfi, tfi, 100000)
estimates

mean(csfi) - mean(tfi) #the observed mean difference between the control group and the experimental group, which is 1.9

ci_lb <- estimates[[1]] - 1.96 * estimates[[3]] #calculate the lower bound of the 95% confidence interval
ci_ub <- estimates[[1]] + 1.96 * estimates[[3]] #calculate the upper bound of the 95% confidence interval

confidence_interval <- c(ci_lb, ci_ub) #create the 95% confidence interval of the mean difference between the computer-simulated flight instructions group and the traditional flight instructions group
confidence_interval
```

Our bootstrap estimation produced a bias of 0.004 (0.0039). This means that there is a difference of 0.004 between our estimatation of the mean difference between the two groups and the true mean difference between the two groups. Since this value is very close to zero, it means that our approach almost provided a unbiased estimate. Still, the bias is very minimal. 

The standard error is a measure of the dispersion of sample means around the population mean. Our approach yielded a standard error of 1.04, which is a reasonable value. It gives us an indication that if we would sample all possible samples from our data and calculate the mean difference for all those samples, they are expected to deviate from the true difference just like in a standard normal distribution.

Finally, we provided a 95% confidence interval of our bootstrap distribution. The 95% confidence interval ranges from -0.14 to 3.95. This means that if we sampled all possible samples from our data, then we would expect that 95% of the true differences falls between these values. However, the 95% confidence interval contains 0. This means that the true difference between the means could be 0. That would mean that we do not know which of the groups scores better on average.