---
title: "Computational Inference with R - Assignment 3"
author: "Jannick Akkermans & Lauke Stoel"
date: "13 november 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1: Emperical power calculation

#1

```{r}
set.seed(123)
control_group <- rnorm(50, mean = 150, sd = 15) #sample 50 observations from a normal distribution with a mean of 150 and a standard deviation of 15 to be the control group
control_group
experimental_group <- rnorm(50, mean = 160, sd = 15) #sample 50 observations from a normal distribution with a mean of 160 and a standard deviation of 15 to be the experimental group
t.test(control_group, experimental_group)
```

The p-value from the in-built t-test function is 4.849e-05. Since this is lower than the alpha-level of 0.05, we reject the null hypothesis. Therefore, we accept the alternative hypothesis that the control group and experimental group differ significantly on the exam scores.

#2

```{r}
library(ggplot2)
set.seed(123)
N <- 1000
simulateTtest <- function(N, n1, n2, mu1, mu2, sd1, sd2) {
  p_values <- numeric(N) #create a vector of 0's whose length is equal to N
  for (i in 1:N) { #simulate 1000 t-tests
    control_group <- rnorm(n1, mean = mu1, sd = sd1) #sample the control group
    experimental_group <- rnorm(n2, mean = mu2, sd = sd2) #sample the experimental group
    p_values[i] <- t.test(control_group, experimental_group)$p.value #perform a t-test and save the p-value
  }
  dx <- density(p_values, adjust = 10)
  plot(dx); polygon(c(dx$x[dx$x < 0.05], 0.05), c(dx$y[dx$x < 0.05], 0.05), col = rgb(1, 0, 0, alpha = 0.5), border = "red", main = "") #create a density plot that shows the distribution of the p-values
  hist(p_values, freq = FALSE, breaks = 20, ylim = c(0,20))
  power <- mean(p_values < 0.05) #calculate the simulated power
  list(Simulated_power = power, True_power = power.t.test(50, delta = 10, sd = 15, sig.level = 0.05, type = "two.sample", alternative = "two.sided")$power) #return a list that contains the simulated power and the true power
}

simulateTtest(N, 50, 50, 150, 160, 15, 15)
```

## Part 2: Something about puppies?

- No outliers, so it doesn't matter if permutation or bootstrap
- Bootstrap has better generalizability
- Bootstrap is less exact than permutation with small samples

```{r}
csfi <- c(2,5,5,6,6,7,8,9)
tfi <- c(1,1,2,3,3,4,5,7,7,8)

var(csfi) #4.57
var(tfi) #6.54

boxplot(csfi); boxplot(tfi) #Used to check outliers
```

- No outliers present, so choice is bootstrap test

```{r}
bootstrapTest <- function(group1, group2, B) {
  set.seed(123)
  sample_difference <- mean(group1) - mean(group2) #The difference between the two groups as they are now. This difference is used to calculate the emperical p-value
  N <- length(group1) #size of group 1
  M <- length(group2) #size of group 2
  differences <- numeric(B) #create a vector with 0's with a length equal to the number of repetitions
  for (i in 1:B) {
    bootstrap_sample <- sample(c(group1, group2), replace = TRUE) #sample WITH replacement from the original sample
    tgroup1 <- bootstrap_sample[1:N] 
    tgroup2 <- bootstrap_sample[(N+1):(N+M)]
    differences[i] <- mean(tgroup1) - mean(tgroup2)
  }
  emp_p <- mean(abs(differences) > abs(sample_difference)) #calculate the proportion of absolute mean differences that is greater than the sample mean differences. We use the absolute value since we want to calculate a two-sided p-value.
  hist(differences); abline(v = sample_difference, lwd = 2, col = "red")
  list(Sample_difference = sample_difference, Distribution_mean = mean(differences), Distribution_sd= sd(differences), P_value = emp_p)
}

bootstrapTest(csfi, tfi, 999)
```

```{r}
bootstrapTestnosample <- function(group1, group2, B) {
  set.seed(123)
  sample_difference <- mean(group1) - mean(group2) #The difference between the two groups as they are now. This difference is used to calculate the emperical p-value
  N <- length(group1) #size of group 1
  M <- length(group2) #size of group 2
  differences <- numeric(B) #create a vector with 0's with a length equal to the number of repetitions
  combined_sample <- c(group1, group2)
  for (i in 1:B) {
    indices <- floor(runif(18, min=1, max=19)) #generate 18 random integers representing the indices of the observations to be selected
    bootstrap_sample <- combined_sample[indices]
    tgroup1 <- bootstrap_sample[1:N] 
    tgroup2 <- bootstrap_sample[(N+1):(N+M)]
    differences[i] <- mean(tgroup1) - mean(tgroup2)
  }
  emp_p <- mean(abs(differences) > abs(sample_difference)) #calculate the proportion of absolute mean differences that is greater than the sample mean differences. We use the absolute value since we want to calculate a two-sided p-value.
  hist(differences); abline(v = sample_difference, lwd = 2, col = "red")
  list(Sample_difference = sample_difference, Distribution_mean = mean(differences), Distribution_sd= sd(differences), P_value = emp_p)
}

bootstrapTestnosample(csfi, tfi, 999)
```

```{r}
set.seed(123)
table(sample(c(csfi, tfi), replace = TRUE))
table(c(csfi, tfi)[dnorm(19, mean = mean(c(csfi, tfi)), sd = sd(c(csfi, tfi)))])
```